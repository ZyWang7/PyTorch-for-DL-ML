{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. PyTorch Paper Replicating\n",
    "\n",
    "### Milestone Project 2: PyTorch Paper Replicating\n",
    "\n",
    "=> replicating a Machine Learning research paper and creating a Vision Transformer(ViT) from scrach using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • What is paper replicating?\n",
    "\n",
    "=> many of advances get published in machine learning research papers.\n",
    "\n",
    "**=> Goal of paper replicating:** take replicate advances with code -> use the techniques for own problem\n",
    "\n",
    "**=> Involves:** *turn a machine learning paper comprised of images/digrams, math and test into usable code and in this case, usable PyTorch code. Digram, math equations and test from the `ViT paper.`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • What is a machine learning research paper?\n",
    "> (1) **Abstract** -> An overview/summary of the paper's main findings/contributions\n",
    ">\n",
    "> (2) **Introduction** -> What's the paper's main problem and details of previous methods used to try and solve it.\n",
    ">\n",
    "> (3) **Method** -> How did the researchers go about conducting their research? -> what model(s), data sources, training setups were used?\n",
    ">\n",
    "> (4) **Results** -> outcomes -> If a new type of model or training setup was used, how did the results of findings compare to previous works?\n",
    ">\n",
    "> (5) **Conclusion** -> limitations of the suggested methods? next steps for the research community?\n",
    ">\n",
    "> (6) **References** -> resources/other papers did the researchers look at to build their own body of work?\n",
    ">\n",
    "> (7) **Appendix** -> any extra resources/findings to look at\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Where to find code examples for ML research paper?\n",
    "> (1) **arXiv** -> a free and open resource for reading technical articles on everything from physics to computer science\n",
    ">\n",
    "> (2) **AK Twitter** -> The AK Twitter account publishes machine learning research highlights, often with live demos almost every day\n",
    ">\n",
    "> (3) **Paper with Code** -> collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models.\n",
    ">\n",
    "> (4) **lucidrains' `vit-pytorch` GitHub repository** -> Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale and with a specific focus looks like. \n",
    ">\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Get Setup\n",
    "\n",
    "=> replicate the machine learning research paper `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale` (ViT paper) with PyTorch - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "=> The `Transformer neural network architecture` was originally introduced in the machine learning research paper `Attention is all you need` - https://arxiv.org/abs/1706.03762\n",
    "\n",
    "=> A `Transformer architecture` is generally considered to be any neural network that uses the `attention mechanism` as its **primary learning layer**. Similar to a how a convolutional neural network (CNN) uses convolutions as its primary learning layer.\n",
    "\n",
    "=> the `Vision Transformer (ViT) architecture` was designed to adapt the original Transformer architecture to vision problem(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from go_modular import data_setup, engine\n",
    "from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "\n",
    "device = \"gpu\" if torch.cuda.is_available() \\\n",
    "    else \"mps\" if torch.backends.mps.is_built() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
